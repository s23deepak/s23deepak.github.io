<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Blog - How vLLM Solves Out-Of-Memory Errors in LLMs</title>
    <meta name="description" content="Exploring the world of AI, one learning at a time.">
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 1rem 0;
            text-align: center;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        header h1 {
            margin: 0;
            font-size: 2.5rem;
        }
        nav {
            margin-top: 1rem;
        }
        nav a {
            color: white;
            text-decoration: none;
            margin: 0 1rem;
            padding: 0.5rem 1rem;
            border-radius: 5px;
            transition: background-color 0.3s;
        }
        nav a:hover {
            background-color: rgba(255,255,255,0.2);
        }
        main {
            max-width: 800px;
            margin: 2rem auto;
            padding: 0 1rem;
            background: white;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
            padding: 2rem;
        }
        footer {
            text-align: center;
            padding: 1rem 0;
            background: #333;
            color: white;
            margin-top: 2rem;
        }
        h1, h2, h3 {
            color: #667eea;
        }
        a {
            color: #667eea;
        }
        a:hover {
            text-decoration: underline;
        }
        .post-list {
            list-style: none;
            padding: 0;
        }
        .post-list li {
            margin-bottom: 1rem;
        }
        .post-list a {
            text-decoration: none;
            font-size: 1.2rem;
        }
        .post-list .date {
            color: #666;
            font-size: 0.9rem;
        }
    </style>
</head>
<body>
    <header>
        <h1>AI Blog</h1>
        <nav>
            <a href="/">Home</a>
            <a href="/about">About</a>
        </nav>
    </header>
    <main>
        <article>
    <header>
        <h1>How vLLM Solves Out-Of-Memory Errors in LLMs</h1>
        <p>Posted on November 14, 2025</p>
        
        <p>Categories: ai, llm, vllm</p>
        
    </header>
    <div>
        <h1 id="how-vllm-solves-out-of-memory-errors-in-llms">How vLLM Solves Out-Of-Memory Errors in LLMs</h1>

<h2 id="understanding-the-problem-kv-cache-memory-challenges">Understanding the Problem: KV Cache Memory Challenges</h2>

<p>During LLM inference, the model stores key-value (KV) representations of all previously generated tokens in memory. As sequences generate more tokens, this KV cache grows dynamically, creating significant memory management challenges that traditional systems struggle to handle efficiently.</p>

<h3 id="three-types-of-memory-waste">Three Types of Memory Waste</h3>

<p><img src="memory_waste.png" alt="alt text" /></p>

<p>Traditional memory management for LLM inference suffers from three critical inefficiencies:</p>

<p><strong>1. Internal Fragmentation</strong><br />
Systems must pre-allocate memory slots for the maximum possible sequence length since the final output length is unknown in advance. When sequences end earlier than expected, the unused pre-allocated space is wasted.</p>

<p><strong>2. Reservation Overhead</strong><br />
Memory slots are allocated but remain idle while waiting for future tokens to be generated. This happens because traditional systems require contiguous memory blocks for each sequence, forcing early allocation of space that may not be immediately needed.</p>

<p><strong>3. External Fragmentation</strong><br />
Different requests have varying sequence lengths, creating unusable gaps between allocated memory blocks. These gaps cannot be efficiently utilized by other sequences, leading to wasted memory capacity.</p>

<h2 id="the-solution-pagedattention">The Solution: PagedAttention</h2>

<p><img src="pagedattention_solution.png" alt="alt text" /></p>

<p>vLLM introduces PagedAttention, which fundamentally reimagines how KV cache memory is managed. Instead of requiring contiguous memory blocks, PagedAttention partitions the KV cache into fixed-size blocks, allowing sequences to occupy non-contiguous memory locations—similar to how operating systems manage virtual memory.</p>

<h3 id="core-components">Core Components</h3>

<p><strong>KV Blocks</strong><br />
The KV cache is divided into fixed-size blocks (typically 16-32 tokens each), analogous to memory pages in operating systems. Each block stores key-value vectors for a fixed number of tokens.</p>

<p><strong>Block Tables</strong><br />
Each sequence maintains a block table that maps logical blocks to physical memory blocks, similar to page tables in OS memory management. This mapping enables non-contiguous storage while maintaining logical sequence ordering.</p>

<p><strong>On-Demand Allocation</strong><br />
Physical blocks are allocated only when needed as sequences grow, rather than pre-allocating the maximum possible length. When a logical block fills up, vLLM allocates a new physical block from the free pool on demand.</p>

<h3 id="how-it-works-in-practice">How It Works in Practice</h3>

<p>During attention computation, PagedAttention efficiently fetches KV blocks from arbitrary memory positions and patches them together for processing. The algorithm operates purely at the memory management level without requiring any model architecture changes.</p>

<p>For example, consider the prompt “Alan Turing is a computer scientist and mathematician.” This gets partitioned into logical blocks like “Alan Turing is a” and “computer scientist and mathematician.” These logical blocks map to physical blocks that may be scattered across GPU memory. As the model generates new tokens, they’re appended to existing blocks (like “renowned”) or trigger allocation of new blocks on demand.</p>

<h2 id="benefits-preventing-out-of-memory-errors">Benefits: Preventing Out-of-Memory Errors</h2>

<p><strong>Dynamic Resource Management</strong><br />
Instead of failing when a single large contiguous block isn’t available, vLLM can utilize any available blocks scattered across memory. The system dynamically trades sequence length for batch size based on actual available memory.</p>

<p><strong>Minimal Fragmentation</strong><br />
Internal fragmentation is bounded by block size (16-32 tokens) rather than maximum sequence length. External fragmentation is eliminated entirely since all blocks have uniform size.</p>

<p><strong>Predictable Memory Usage</strong><br />
The total physical cache memory remains statically allocated, providing protection against runtime OOM errors. The scheduler intelligently manages which requests advance based on available blocks in the free pool.</p>

<p><strong>Efficient Cleanup</strong><br />
When sequences complete, their blocks immediately return to the free pool for reuse by other requests, maximizing memory utilization.</p>

<p>Photos courtesy of <a href="https://www.youtube.com/watch?v=5ZlavKF_98U">Fast LLM Serving with vLLM and PagedAttention</a></p>

    </div>
</article>
    </main>
    <footer>
        <p>&copy; 2025 AI Blog. Powered by Jekyll.</p>
    </footer>
</body>
</html>