<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://0.0.0.0:4001/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4001/" rel="alternate" type="text/html" /><updated>2025-11-15T03:19:27+00:00</updated><id>http://0.0.0.0:4001/feed.xml</id><title type="html">AI Blog</title><subtitle>Exploring the world of AI, one learning at a time.</subtitle><entry><title type="html">How vLLM Solves Out-Of-Memory Errors in LLMs</title><link href="http://0.0.0.0:4001/ai/llm/vllm/2025/11/14/how-vllm-solves-out-of-memory-errors-in-llms.html" rel="alternate" type="text/html" title="How vLLM Solves Out-Of-Memory Errors in LLMs" /><published>2025-11-14T00:00:00+00:00</published><updated>2025-11-14T00:00:00+00:00</updated><id>http://0.0.0.0:4001/ai/llm/vllm/2025/11/14/how-vllm-solves-out-of-memory-errors-in-llms</id><content type="html" xml:base="http://0.0.0.0:4001/ai/llm/vllm/2025/11/14/how-vllm-solves-out-of-memory-errors-in-llms.html"><![CDATA[<h1 id="how-vllm-solves-out-of-memory-errors-in-llms">How vLLM Solves Out-Of-Memory Errors in LLMs</h1>

<h2 id="understanding-the-problem-kv-cache-memory-challenges">Understanding the Problem: KV Cache Memory Challenges</h2>

<p>During LLM inference, the model stores key-value (KV) representations of all previously generated tokens in memory. As sequences generate more tokens, this KV cache grows dynamically, creating significant memory management challenges that traditional systems struggle to handle efficiently.</p>

<h3 id="three-types-of-memory-waste">Three Types of Memory Waste</h3>

<p><img src="memory_waste.png" alt="alt text" /></p>

<p>Traditional memory management for LLM inference suffers from three critical inefficiencies:</p>

<p><strong>1. Internal Fragmentation</strong><br />
Systems must pre-allocate memory slots for the maximum possible sequence length since the final output length is unknown in advance. When sequences end earlier than expected, the unused pre-allocated space is wasted.</p>

<p><strong>2. Reservation Overhead</strong><br />
Memory slots are allocated but remain idle while waiting for future tokens to be generated. This happens because traditional systems require contiguous memory blocks for each sequence, forcing early allocation of space that may not be immediately needed.</p>

<p><strong>3. External Fragmentation</strong><br />
Different requests have varying sequence lengths, creating unusable gaps between allocated memory blocks. These gaps cannot be efficiently utilized by other sequences, leading to wasted memory capacity.</p>

<h2 id="the-solution-pagedattention">The Solution: PagedAttention</h2>

<p><img src="pagedattention_solution.png" alt="alt text" /></p>

<p>vLLM introduces PagedAttention, which fundamentally reimagines how KV cache memory is managed. Instead of requiring contiguous memory blocks, PagedAttention partitions the KV cache into fixed-size blocks, allowing sequences to occupy non-contiguous memory locations—similar to how operating systems manage virtual memory.</p>

<h3 id="core-components">Core Components</h3>

<p><strong>KV Blocks</strong><br />
The KV cache is divided into fixed-size blocks (typically 16-32 tokens each), analogous to memory pages in operating systems. Each block stores key-value vectors for a fixed number of tokens.</p>

<p><strong>Block Tables</strong><br />
Each sequence maintains a block table that maps logical blocks to physical memory blocks, similar to page tables in OS memory management. This mapping enables non-contiguous storage while maintaining logical sequence ordering.</p>

<p><strong>On-Demand Allocation</strong><br />
Physical blocks are allocated only when needed as sequences grow, rather than pre-allocating the maximum possible length. When a logical block fills up, vLLM allocates a new physical block from the free pool on demand.</p>

<h3 id="how-it-works-in-practice">How It Works in Practice</h3>

<p>During attention computation, PagedAttention efficiently fetches KV blocks from arbitrary memory positions and patches them together for processing. The algorithm operates purely at the memory management level without requiring any model architecture changes.</p>

<p>For example, consider the prompt “Alan Turing is a computer scientist and mathematician.” This gets partitioned into logical blocks like “Alan Turing is a” and “computer scientist and mathematician.” These logical blocks map to physical blocks that may be scattered across GPU memory. As the model generates new tokens, they’re appended to existing blocks (like “renowned”) or trigger allocation of new blocks on demand.</p>

<h2 id="benefits-preventing-out-of-memory-errors">Benefits: Preventing Out-of-Memory Errors</h2>

<p><strong>Dynamic Resource Management</strong><br />
Instead of failing when a single large contiguous block isn’t available, vLLM can utilize any available blocks scattered across memory. The system dynamically trades sequence length for batch size based on actual available memory.</p>

<p><strong>Minimal Fragmentation</strong><br />
Internal fragmentation is bounded by block size (16-32 tokens) rather than maximum sequence length. External fragmentation is eliminated entirely since all blocks have uniform size.</p>

<p><strong>Predictable Memory Usage</strong><br />
The total physical cache memory remains statically allocated, providing protection against runtime OOM errors. The scheduler intelligently manages which requests advance based on available blocks in the free pool.</p>

<p><strong>Efficient Cleanup</strong><br />
When sequences complete, their blocks immediately return to the free pool for reuse by other requests, maximizing memory utilization.</p>

<p>Photos courtesy of <a href="https://www.youtube.com/watch?v=5ZlavKF_98U">Fast LLM Serving with vLLM and PagedAttention</a></p>]]></content><author><name></name></author><category term="ai" /><category term="llm" /><category term="vllm" /><summary type="html"><![CDATA[How vLLM Solves Out-Of-Memory Errors in LLMs]]></summary></entry><entry><title type="html">AlphaZero with JAX-based Implementation</title><link href="http://0.0.0.0:4001/ai/alphazero/jax/2025/09/15/alphazero-with-jax-based-implementation.html" rel="alternate" type="text/html" title="AlphaZero with JAX-based Implementation" /><published>2025-09-15T00:00:00+00:00</published><updated>2025-09-15T00:00:00+00:00</updated><id>http://0.0.0.0:4001/ai/alphazero/jax/2025/09/15/alphazero-with-jax-based-implementation</id><content type="html" xml:base="http://0.0.0.0:4001/ai/alphazero/jax/2025/09/15/alphazero-with-jax-based-implementation.html"><![CDATA[<h1 id="alphazero-with-jax-based-implementation">AlphaZero with JAX-based Implementation</h1>

<p>This directory contains an implementation of AlphaZero for a custom turn-based strategy game, using <a href="https://jax.readthedocs.io/en/latest/">JAX</a> for high-performance computation and parallelization. Monte Carlo Tree Search (MCTS) is combined with a convolutional neural network (CNN) for policy and value prediction. JAX is used to accelerate neural network operations and enable efficient parallelization across devices like GPUs and TPUs.</p>

<h2 id="how-it-works">How It Works</h2>

<ul>
  <li><strong>JAX</strong> provides just-in-time (JIT) compilation and automatic differentiation to speed up computations and enable efficient batching and parallelization.</li>
  <li><strong>MCTX</strong> By <a href="https://github.com/google-deepmind/mctx">DeepMind</a>, is Monte Carlo Tree Search implementation in JAX</li>
  <li><strong>Neural network inference and training</strong> are performed on the GPU/TPU using JAX’s optimized primitives.</li>
</ul>

<h3 id="jax-jit-and-vectorization">JAX JIT and Vectorization</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">@jax.jit</code>: Compiles the function for efficient execution on CPU/GPU/TPU, eliminating Python overhead.</li>
  <li><code class="language-plaintext highlighter-rouge">jax.vmap</code>: Automatically vectorizes the function to handle batches of inputs efficiently, enabling parallel processing of multiple game states.</li>
</ul>

<h4 id="effect-on-utilization">Effect on Utilization</h4>

<ul>
  <li><strong>CPU Utilization</strong>: MCTS remains CPU-bound, but JAX’s JIT compilation reduces overhead in neural network calls.</li>
  <li><strong>GPU/TPU Utilization</strong>: JAX enables high utilization by compiling operations to optimized kernels and supporting batch processing. Multiple MCTS simulations can be batched together for inference, improving hardware efficiency.</li>
  <li><strong>Parallelization</strong>: JAX’s <code class="language-plaintext highlighter-rouge">pmap</code> can distribute computations across multiple devices if available, further increasing throughput.</li>
</ul>

<p><strong>Note:</strong> JAX’s functional programming paradigm requires careful state management and <a href="https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions">pure functions</a>, but it provides significant performance gains for numerical computations in AlphaZero.</p>

<h2 id="observations">Observations</h2>

<ul>
  <li><strong>GPU/TPU Utilization</strong>: JAX maximizes hardware utilization through compilation and vectorization, making neural network operations much faster than in pure NumPy implementations.</li>
  <li><strong>Scalability</strong>: JAX’s design allows for easy scaling to multiple GPUs or TPUs, providing better performance than CPU-only implementations.</li>
</ul>]]></content><author><name></name></author><category term="ai" /><category term="alphazero" /><category term="jax" /><summary type="html"><![CDATA[AlphaZero with JAX-based Implementation]]></summary></entry></feed>